{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGNN - Triad Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import operator\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "import os\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the dataset, the type of prediction and the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = 'cora'\n",
    "PREDICTION = 'triad'\n",
    "RUN_COUNT = 1\n",
    "NUM_SAMPLES = 1\n",
    "PATH_TO_DATASETS_DIRECTORY = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'reddit': Reddit(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Reddit'),\n",
    "    'cora' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Cora/', name='Cora'),\n",
    "    'citeseer' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/CiteSeer/', name='CiteSeer'),\n",
    "    'pubmed' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/PubMed/', name='PubMed'),\n",
    "}\n",
    "dataset = datasets[DATASET]\n",
    "data = dataset[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'node' : dataset.num_classes,\n",
    "    'link' : 2,\n",
    "    'triad' : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = ['train', 'validation', 'test']\n",
    "triads_stores = dict()\n",
    "triad_loc = '/scratch-ml00/bsriniv/pos2struct/triad_store/'\n",
    "\n",
    "for set_nature in dataset_types :\n",
    "    zero_filename = triad_loc + set_nature + '/' + DATASET + '_triad_zero.pickle'\n",
    "    one_filename = triad_loc + set_nature + '/' + DATASET + '_triad_one.pickle'\n",
    "    two_filename = triad_loc + set_nature + '/' + DATASET + '_triad_two.pickle'\n",
    "    three_filename = triad_loc + set_nature + '/' + DATASET + '_triad_three.pickle'\n",
    "    with open(zero_filename, 'rb') as f:\n",
    "        zeros = pickle.load(f)\n",
    "    with open(one_filename, 'rb') as f:\n",
    "        ones = pickle.load(f)\n",
    "    with open(two_filename, 'rb') as f:\n",
    "        twos = pickle.load(f)\n",
    "    with open(three_filename, 'rb') as f:\n",
    "        threes = pickle.load(f)\n",
    "    triads_stores[set_nature] = dict()\n",
    "    triads_stores[set_nature]['zeros'] = zeros\n",
    "    triads_stores[set_nature]['ones'] = ones\n",
    "    triads_stores[set_nature]['twos'] = twos\n",
    "    triads_stores[set_nature]['threes'] = threes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Splash Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_rho_hidden = 256\n",
    "ds_rho_visible = dataset.num_features\n",
    "num_neurons = 256\n",
    "\n",
    "\n",
    "def parallel_splash_variant(adjacency_matrix, num_splashes=64):\n",
    "    \"\"\"\n",
    "        Parallel Splash Gibbs Sampling\n",
    "    \"\"\"\n",
    "    max_value = adjacency_matrix.shape[0]\n",
    "    max_value_hidden = adjacency_matrix.shape[1]\n",
    "    batches = dict()\n",
    "    neighborhood_tracker = dict()\n",
    "    curr_step_max_len = 0\n",
    "    num_batches = 0\n",
    "    all_nodes = list(range(max_value))\n",
    "    root_allowed_nodes = set(all_nodes)\n",
    "    #Check if all nodes have been reached\n",
    "    #Strong Assumption that connected graph or all connected components will be seen atleast once, if not batch size will be increased\n",
    "    for i in range(num_splashes):\n",
    "        temp_list = list(root_allowed_nodes)\n",
    "        shuffle(temp_list)\n",
    "        root_node = temp_list.pop()\n",
    "        root_allowed_nodes = set(temp_list)\n",
    "        batches[root_node] = [root_node]\n",
    "        root_node_neighbors = set(sum(torch.LongTensor((adjacency_matrix[root_node]).nonzero()).tolist(),[]))\n",
    "        neighborhood_tracker[root_node] = dict((k,1) for k in root_node_neighbors)\n",
    "        root_allowed_nodes = root_allowed_nodes - root_node_neighbors\n",
    "        if len(root_allowed_nodes) == 0 :\n",
    "            # print(\"Unable to create %d splashes due to graph connectivity or number of nodes\"%num_splashes)\n",
    "            break\n",
    "    allowed_nodes = set(all_nodes) - set(batches.keys())\n",
    "    prev_size  = len(allowed_nodes)\n",
    "    while len(allowed_nodes) > 0:\n",
    "        for root in batches:\n",
    "            #Have to grow splash one at a time for each splash\n",
    "            #Select node with max number of shared neighbors\n",
    "            try :\n",
    "                node_selected = max(neighborhood_tracker[root].items(), key=operator.itemgetter(1))[0]\n",
    "                node_neighbors = set(sum(torch.LongTensor(adjacency_matrix[node_selected].nonzero()).tolist(),[]))\n",
    "                batches[root].insert(0, node_selected)\n",
    "                #Clear up, so that it doesn't show up in other splashes\n",
    "                allowed_nodes.remove(node_selected)\n",
    "                #Decrement / Increment neighbor count for other/ current root\n",
    "                for r in batches:\n",
    "                    if node_selected in neighborhood_tracker[r]:\n",
    "                        del neighborhood_tracker[r][node_selected]\n",
    "                        if root!=r:\n",
    "                            for neigh in node_neighbors:\n",
    "                                if neigh in neighborhood_tracker[r]:\n",
    "                                    neighborhood_tracker[r][neigh]-=1\n",
    "                        else:\n",
    "                             for neigh in node_neighbors :\n",
    "                                if neigh in neighborhood_tracker[r]:\n",
    "                                    neighborhood_tracker[r][neigh]+=1\n",
    "                                elif neigh not in neighborhood_tracker[r] and neigh in allowed_nodes :\n",
    "                                    neighborhood_tracker[r][neigh]=1\n",
    "            except :\n",
    "                pass\n",
    "        curr_size = len(allowed_nodes)\n",
    "        if curr_size == prev_size and curr_size!=0:\n",
    "            #Add a random node and increase the batch size by 1\n",
    "            new_root_node = allowed_nodes.pop()\n",
    "            batches[new_root_node] = [new_root_node]\n",
    "            new_root_node_neighbors = set(sum(torch.LongTensor((adjacency_matrix[new_root_node]==0).nonzero()).tolist(),[]))\n",
    "            neighborhood_tracker[new_root_node] = dict((k,1) for k in new_root_node_neighbors)\n",
    "            curr_size-=1\n",
    "        prev_size = curr_size\n",
    "\n",
    "    for root in batches:\n",
    "        num_batches = max(num_batches, len(batches[root]))\n",
    "    batches_ = dict((k,[]) for k in range(num_batches))\n",
    "    batch_info = dict((k,[]) for k in range(num_batches))\n",
    "    for i in range(num_batches):\n",
    "        curr_step_max_len = 0\n",
    "        for root in batches:\n",
    "            try:\n",
    "                batches_[i].append(batches[root][i])\n",
    "                curr_node_neigh = sum(torch.LongTensor(adjacency_matrix[batches[root][i]].nonzero()).tolist(),[])\n",
    "                cn_neighbor = set(curr_node_neigh)\n",
    "                b_neighbor = set(batches[root])\n",
    "                curr_node_neigh = list(cn_neighbor.intersection(b_neighbor)) #list(cn_neighbor)\n",
    "                batch_info[i].append(curr_node_neigh)\n",
    "                curr_step_max_len = max(curr_step_max_len, len(curr_node_neigh))\n",
    "            except :\n",
    "                pass\n",
    "        for node_neigh in batch_info[i]:\n",
    "            while len(node_neigh) < curr_step_max_len :\n",
    "                node_neigh.append(max_value_hidden)\n",
    "\n",
    "    return batches_, batch_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colliders Learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ColliderNetworks(nn.Module):\n",
    "    def __init__(self, weight_norm=True):\n",
    "        super(ColliderNetworks, self).__init__()\n",
    "        #Deepsets MLP's\n",
    "        self.rho_mlp_visible_1 = nn.Linear(ds_rho_visible, num_neurons)\n",
    "        self.rho_mlp_visible_2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.rho_mlp_visible_1_dropout = nn.Dropout(p=0.5)\n",
    "        self.rho_mlp_hidden_1 = nn.Linear(ds_rho_hidden, num_neurons)\n",
    "        self.rho_mlp_hidden_2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.rho_mlp_hidden_1_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        #Gibbs Sampling MLP's - Mean\n",
    "        self.colliders_mlp_1 = nn.Linear(2*num_neurons, num_neurons)\n",
    "        self.colliders_mlp_2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.colliders_mlp_1_dropout_mean = nn.Dropout(p=0.5)\n",
    "        self.colliders_mlp_1_dropout_variance = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.relu_activation = nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        self.loss_func = nn.BCELoss()\n",
    "\n",
    "        self.rho_mlp_visible_1_norm = nn.BatchNorm1d(num_neurons)\n",
    "        self.rho_mlp_hidden_1_norm = nn.BatchNorm1d(num_neurons)\n",
    "        self.colliders_mlp_1_norm = nn.BatchNorm1d(num_neurons)\n",
    "\n",
    "        #Xavier Uniform Inits, Biases set to zero\n",
    "        init.xavier_uniform_(self.rho_mlp_visible_1.weight)\n",
    "        init.xavier_uniform_(self.rho_mlp_visible_2.weight)\n",
    "        init.xavier_uniform_(self.rho_mlp_hidden_1.weight)\n",
    "        init.xavier_uniform_(self.rho_mlp_hidden_2.weight)\n",
    "        init.xavier_uniform_(self.colliders_mlp_1.weight)\n",
    "        init.xavier_uniform_(self.colliders_mlp_2.weight)\n",
    "        self.rho_mlp_visible_1.bias.data.fill_(0)\n",
    "        self.rho_mlp_visible_2.bias.data.fill_(0)\n",
    "        self.rho_mlp_hidden_1.bias.data.fill_(0)\n",
    "        self.rho_mlp_hidden_2.bias.data.fill_(0)\n",
    "        self.colliders_mlp_1.bias.data.fill_(0)\n",
    "        self.colliders_mlp_2.bias.data.fill_(0)\n",
    "\n",
    "    def deepsets(self, input_tensor, hidden=False):\n",
    "        \"\"\"\n",
    "            Returns the set representation of the input\n",
    "            rho uses an mlp, Activation is relu\n",
    "        \"\"\"\n",
    "        deepsets_sum = torch.sum(input_tensor, dim = 1)\n",
    "        if hidden :\n",
    "            out = self.rho_mlp_hidden_1(deepsets_sum)\n",
    "            try:\n",
    "                out = self.rho_mlp_hidden_1_norm(out)\n",
    "            except:\n",
    "                pass\n",
    "            out = self.relu_activation(out)\n",
    "            out = self.rho_mlp_hidden_1_dropout(out)\n",
    "            out = self.rho_mlp_hidden_2(out)\n",
    "        else:\n",
    "            out = self.rho_mlp_visible_1(deepsets_sum)\n",
    "            try:\n",
    "                out = self.rho_mlp_visible_1_norm(out)\n",
    "            except:\n",
    "                pass\n",
    "            out = self.relu_activation(out)\n",
    "            out = self.rho_mlp_visible_1_dropout(out)\n",
    "            out = self.rho_mlp_visible_2(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, adjacency_matrix, hidden_embeddings, visible_feats, batches, batch_info, num_times=5):\n",
    "        #Torch Embeddings, one for visible and another for hidden\n",
    "        #Call Gibbs Sampling multiple times\n",
    "        for iteration in range(num_times):\n",
    "            self.gibbs_sampling(hidden_embeddings, visible_feats, batches, batch_info)\n",
    "        return hidden_embeddings\n",
    "\n",
    "    def reconstruction_loss(self, adjacency_matrix, hidden_embeddings):\n",
    "        \"\"\"\n",
    "            Reconstrunction Loss: Compute loss over all n choose 2 possible edges, non edges\n",
    "        \"\"\"\n",
    "        #Multiply the hidden embedding tensor with its transpose and compute sigmoid\n",
    "        max_value = adjacency_matrix.shape[0]\n",
    "        if max_value > 5000 :\n",
    "            selection = torch.randint(low=0, high=2,size=(max_value,1)).type(torch.uint8).view(-1)\n",
    "            adjacency_matrix = adjacency_matrix[selection].t()[selection].t()\n",
    "            hidden_embeddings = hidden_embeddings[:max_value][selection]\n",
    "        if max_value < 5000 :\n",
    "            edge_probability = torch.matmul(hidden_embeddings[:max_value], torch.t(hidden_embeddings[:max_value]))\n",
    "        else :\n",
    "            edge_probability = torch.matmul(hidden_embeddings, torch.t(hidden_embeddings))\n",
    "        edge_probability = torch.sigmoid(edge_probability)\n",
    "        preds = edge_probability.view(-1)\n",
    "        target = torch.reshape(adjacency_matrix, (-1,1)).view(-1).to(device).type(torch.float)\n",
    "        #target = adjacency_matrix.view(-1).to(device).type(torch.float)\n",
    "        target_inverse = 1.0 - target\n",
    "        total_edges_non_edges = max_value**2\n",
    "        num_non_edges = (target == 0).sum().item()\n",
    "        num_edges = total_edges_non_edges - num_non_edges\n",
    "        class_weights = target*(num_non_edges/total_edges_non_edges) + target_inverse*(num_edges/ total_edges_non_edges)\n",
    "        loss = F.binary_cross_entropy(preds, target, weight=class_weights)\n",
    "        return loss\n",
    "\n",
    "    def gibbs_sampling(self, hidden_embeddings, visible_feats, batches, batch_info):\n",
    "        \"\"\"\n",
    "        Runs one complete iteration of Gibbs Sampling\n",
    "        \"\"\"\n",
    "        #For each of the constructed batches\n",
    "        for num_batch in range(len(batches)):\n",
    "            #Pass the hidden and visible separately through deepsets and concatenate them\n",
    "            hidden_dependencies = hidden_embeddings[torch.LongTensor(batch_info[num_batch]).to(device)]\n",
    "            visible_dependencies = visible_feats[torch.LongTensor(batch_info[num_batch]).to(device)]\n",
    "            hidden_dependencies = hidden_dependencies.detach()\n",
    "            visible_dependencies = visible_dependencies.detach()\n",
    "\n",
    "            set_rep_hidden  = self.deepsets(hidden_dependencies, hidden=True)\n",
    "            set_rep_visible = self.deepsets(visible_dependencies, hidden=False)\n",
    "            posterior = torch.cat((set_rep_hidden, set_rep_visible), 1)\n",
    "            posterior = posterior.detach()\n",
    "\n",
    "            #Perform a sampling equivalent using a MLP\n",
    "            out = self.colliders_mlp_1(posterior)\n",
    "            try:\n",
    "                out = self.colliders_mlp_1_norm(out)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            out = self.relu_activation(out)\n",
    "\n",
    "            mean = self.colliders_mlp_1_dropout_mean(out)\n",
    "            var = self.colliders_mlp_1_dropout_variance(out)\n",
    "            m = Normal(torch.zeros(var.shape), torch.ones(var.shape))\n",
    "            noise = torch.Tensor(m.sample()).to(device)\n",
    "            out = mean + torch.mul(noise, var)\n",
    "\n",
    "            out = self.colliders_mlp_2(out)\n",
    "            hidden_embeddings[batches[num_batch]] = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the non-overlapping induced subgraphs and Corrupt a small fraction of the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask = 1 - data.val_mask - data.test_mask\n",
    "\n",
    "adj_mat = torch.zeros((data.num_nodes,data.num_nodes)).type(torch.short)\n",
    "edges = data.edge_index.t()\n",
    "adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "\n",
    "adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()\n",
    "\n",
    "\n",
    "def corrupt_adj(adj_mat, task, percent=1):\n",
    "    \"\"\" Returns the corrupted version of the adjacency matrix \"\"\"\n",
    "    if task == 'link':\n",
    "        edges = adj_mat.triu().nonzero()\n",
    "        num_edges = edges.shape[0]\n",
    "        num_to_corrupt = int(percent/100.0 * num_edges)\n",
    "        random_corruption = np.random.randint(num_edges, size=num_to_corrupt)\n",
    "        adj_mat_corrupted = adj_mat.clone()\n",
    "        false_edges, false_non_edges = [], []\n",
    "        #Edge Corruption\n",
    "        for ed in edges[random_corruption]:\n",
    "            adj_mat_corrupted[ed[0], ed[1]] = 0\n",
    "            adj_mat_corrupted[ed[1], ed[0]] = 0\n",
    "            false_non_edges.append(ed.tolist())\n",
    "        #Non Edge Corruption\n",
    "        random_non_edge_corruption = list(np.random.randint(adj_mat.shape[0], size = 6*num_to_corrupt))\n",
    "        non_edge_to_corrupt = []\n",
    "        for k in range(len(random_non_edge_corruption)-1):\n",
    "            to_check = [random_non_edge_corruption[k], random_non_edge_corruption[k+1]]\n",
    "            if to_check not in edges.tolist():\n",
    "                non_edge_to_corrupt.append(to_check)\n",
    "            if len(non_edge_to_corrupt) == num_to_corrupt:\n",
    "                break\n",
    "        non_edge_to_corrupt = torch.Tensor(non_edge_to_corrupt).type(torch.int16)\n",
    "        for n_ed in non_edge_to_corrupt:\n",
    "            adj_mat_corrupted[n_ed[0], n_ed[1]] = 1\n",
    "            adj_mat_corrupted[n_ed[1], n_ed[0]] = 1\n",
    "            false_edges.append(n_ed.tolist())\n",
    "    return adj_mat_corrupted, false_edges, false_non_edges\n",
    "\n",
    "\n",
    "\n",
    "adj_train_corrupted, train_false_edges, train_false_non_edges = corrupt_adj(adj_train, 'link', percent=1)\n",
    "adj_val_corrupted, val_false_edges, val_false_non_edges = corrupt_adj(adj_validation, 'link', percent=1)\n",
    "adj_test_corrupted, test_false_edges, test_false_non_edges  = corrupt_adj(adj_test, 'link', percent=1)\n",
    "\n",
    "\n",
    "train_batches, train_batch_info = parallel_splash_variant(adj_train_corrupted, 64)\n",
    "val_batches, val_batch_info = parallel_splash_variant(adj_val_corrupted, 64)\n",
    "test_batches, test_batch_info = parallel_splash_variant(adj_test_corrupted, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Collider Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_feats = data.x.to(device)\n",
    "temp_holder = torch.zeros((1, data.num_features)).to(device)\n",
    "visible_feats = torch.cat((visible_feats, temp_holder))\n",
    "validation_loss = 10000.0\n",
    "torch.cuda.empty_cache()\n",
    "collider_sample = ColliderNetworks().to(device)\n",
    "colliders_model = 'best_colliders_model.model'\n",
    "optimizer = torch.optim.Adam(collider_sample.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):\n",
    "    print(\"Epoch Num: \", epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer.zero_grad()\n",
    "    normal_init = Normal(torch.zeros(adj_train_corrupted.shape[0]+1, ds_rho_hidden), torch.ones(adj_train_corrupted.shape[0]+1, ds_rho_hidden))\n",
    "    hidden_embeddings = torch.Tensor(normal_init.sample()).to(device)\n",
    "    hidden_embeddings[-1] = torch.zeros((1, ds_rho_hidden)).to(device)\n",
    "    hidden_embeddings = hidden_embeddings.detach()\n",
    "    visible_feats = data.x[data.train_mask].to(device)\n",
    "    temp_holder = torch.zeros((1, data.num_features)).to(device)\n",
    "    visible_feats = torch.cat((visible_feats, temp_holder))\n",
    "    hidden_embeddings = collider_sample.forward(adjacency_matrix=adj_train_corrupted, hidden_embeddings=hidden_embeddings, batches=train_batches, batch_info=train_batch_info, num_times=2, visible_feats=visible_feats)\n",
    "    loss = collider_sample.reconstruction_loss(adjacency_matrix=adj_train_corrupted, hidden_embeddings=hidden_embeddings)\n",
    "    loss.backward()\n",
    "    print(\"Training Loss: \", loss.item())\n",
    "    sys.stdout.flush()\n",
    "    with torch.no_grad():\n",
    "        #Do Validation and check if validation loss has gone down\n",
    "        normal_init = Normal(torch.zeros(adj_val_corrupted.shape[0]+1, ds_rho_hidden), torch.ones(adj_val_corrupted.shape[0]+1, ds_rho_hidden))\n",
    "        hidden_embeddings = torch.Tensor(normal_init.sample()).to(device)\n",
    "        hidden_embeddings[-1] = torch.zeros((1, ds_rho_hidden)).to(device)\n",
    "        hidden_embeddings = hidden_embeddings.detach()\n",
    "        visible_feats = data.x[data.val_mask].to(device)\n",
    "        temp_holder = torch.zeros((1, data.num_features)).to(device)\n",
    "        visible_feats = torch.cat((visible_feats, temp_holder))\n",
    "        hidden_embeddings = collider_sample.forward(adjacency_matrix=adj_val_corrupted, hidden_embeddings=hidden_embeddings, batches=val_batches, batch_info=val_batch_info, num_times=2, visible_feats=visible_feats)\n",
    "        compute_val_loss = collider_sample.reconstruction_loss(adjacency_matrix=adj_val_corrupted, hidden_embeddings=hidden_embeddings)\n",
    "        if compute_val_loss < validation_loss:\n",
    "            validation_loss = compute_val_loss\n",
    "            print(\"Validation Loss: \", validation_loss)\n",
    "            #Save Model\n",
    "            torch.save(collider_sample.state_dict(), colliders_model)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best saved colliders model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collider_sample = ColliderNetworks().to(device)\n",
    "collider_sample.load_state_dict(torch.load(colliders_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Multiple Samples for Train, Validation and Test uing the Colliders Model with different normal inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_samples_train = []\n",
    "hidden_samples_validation = []\n",
    "hidden_samples_test = []\n",
    "for sample in range(NUM_SAMPLES):\n",
    "    print(\"Sample No:: \", sample)\n",
    "    with torch.no_grad():\n",
    "        #Training\n",
    "        normal_init = Normal(torch.zeros(adj_train_corrupted.shape[0]+1, ds_rho_hidden), torch.ones(adj_train_corrupted.shape[0]+1, ds_rho_hidden))\n",
    "        hidden_embeddings = torch.Tensor(normal_init.sample()).to(device)\n",
    "        hidden_embeddings[-1] = torch.zeros((1, ds_rho_hidden)).to(device)\n",
    "        hidden_embeddings = hidden_embeddings.detach()\n",
    "        visible_feats = data.x[data.train_mask].to(device)\n",
    "        temp_holder = torch.zeros((1, data.num_features)).to(device)\n",
    "        visible_feats = torch.cat((visible_feats, temp_holder))\n",
    "        hidden_embeddings = collider_sample.forward(adjacency_matrix=adj_train_corrupted, hidden_embeddings=hidden_embeddings, batches=train_batches, batch_info=train_batch_info, num_times=5, visible_feats=visible_feats)\n",
    "        hidden_samples_train.append(hidden_embeddings)\n",
    "        #Validation\n",
    "        normal_init = Normal(torch.zeros(adj_val_corrupted.shape[0]+1, ds_rho_hidden), torch.ones(adj_val_corrupted.shape[0]+1, ds_rho_hidden))\n",
    "        hidden_embeddings = torch.Tensor(normal_init.sample()).to(device)\n",
    "        hidden_embeddings[-1] = torch.zeros((1, ds_rho_hidden)).to(device)\n",
    "        hidden_embeddings = hidden_embeddings.detach()\n",
    "        visible_feats = data.x[data.val_mask].to(device)\n",
    "        temp_holder = torch.zeros((1, data.num_features)).to(device)\n",
    "        visible_feats = torch.cat((visible_feats, temp_holder))\n",
    "        hidden_embeddings = collider_sample.forward(adjacency_matrix=adj_val_corrupted, hidden_embeddings=hidden_embeddings, batches=val_batches, batch_info=val_batch_info, num_times=5, visible_feats=visible_feats)\n",
    "        hidden_samples_validation.append(hidden_embeddings)\n",
    "        #Test\n",
    "        normal_init = Normal(torch.zeros(adj_test_corrupted.shape[0]+1, ds_rho_hidden), torch.ones(adj_test_corrupted.shape[0]+1, ds_rho_hidden))\n",
    "        hidden_embeddings = torch.Tensor(normal_init.sample()).to(device)\n",
    "        hidden_embeddings[-1] = torch.zeros((1, ds_rho_hidden)).to(device)\n",
    "        hidden_embeddings = hidden_embeddings.detach()\n",
    "        visible_feats = data.x[data.test_mask].to(device)\n",
    "        temp_holder = torch.zeros((1, data.num_features)).to(device)\n",
    "        visible_feats = torch.cat((visible_feats, temp_holder))\n",
    "        hidden_embeddings = collider_sample.forward(adjacency_matrix=adj_test_corrupted, hidden_embeddings=hidden_embeddings, batches=test_batches, batch_info=test_batch_info, num_times=5, visible_feats=visible_feats)\n",
    "        hidden_samples_test.append(hidden_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_SAMPLES):\n",
    "    hidden_samples_train[i] = hidden_samples_train[i][:-1]\n",
    "    hidden_samples_train[i] = torch.cat((hidden_samples_train[i], data.x[data.train_mask].to(device)),1)\n",
    "    hidden_samples_validation[i] = hidden_samples_validation[i][:-1]\n",
    "    hidden_samples_validation[i] = torch.cat((hidden_samples_validation[i], data.x[data.val_mask].to(device)),1)\n",
    "    hidden_samples_test[i] = hidden_samples_test[i][:-1]\n",
    "    hidden_samples_test[i] = torch.cat((hidden_samples_test[i], data.x[data.test_mask].to(device)),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Supervised Learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neurons = 256\n",
    "input_rep = num_neurons + data.num_features\n",
    "\n",
    "class StructMLP(nn.Module):\n",
    "    def __init__(self, node_set_size=1):\n",
    "        super(StructMLP, self).__init__()\n",
    "\n",
    "        self.node_set_size = node_set_size\n",
    "        #Deepsets MLP\n",
    "\n",
    "        self.ds_layer_1 = nn.Linear(input_rep, num_neurons)\n",
    "        self.ds_layer_2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.rho_layer_1 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.rho_layer_2 = nn.Linear(num_neurons, num_neurons)\n",
    "\n",
    "        #One Hidden Layer\n",
    "        self.layer1 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.layer2 = nn.Linear(num_neurons, predictions[PREDICTION])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor, samples):\n",
    "        #Deepsets initially on each of the samples\n",
    "        num_nodes = input_tensor.shape[1]\n",
    "        sum_tensor = torch.zeros(samples.shape[0], num_neurons).to(device)\n",
    "        for i in range(input_tensor.shape[0]):\n",
    "            #Process the input tensor to form n choose k combinations and create a zero tensor\n",
    "            set_init_rep = input_tensor[i].view(-1, input_rep)\n",
    "            x = self.ds_layer_1(set_init_rep)\n",
    "            x = self.relu(x)\n",
    "            x = self.ds_layer_2(x)\n",
    "            x = x[samples]\n",
    "            x = torch.sum(x, dim=1)\n",
    "            x = self.rho_layer_1(x)\n",
    "            sum_tensor += x\n",
    "\n",
    "        x = sum_tensor / input_tensor.shape[0]\n",
    "\n",
    "        #One Hidden Layer for predictor\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, input_tensor, samples, target):\n",
    "        pred = self.forward(input_tensor, samples)\n",
    "        return F.cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PREDICTION == 'node':\n",
    "    node_set_size = 1\n",
    "elif PREDICTION == 'link':\n",
    "    node_set_size = 2\n",
    "else:\n",
    "    node_set_size = 3\n",
    "\n",
    "mlp = StructMLP(node_set_size).to(device)\n",
    "mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "mlp_model = 'best_mlp_model.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Supervised Learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_triads(set_nature, small_samples=100):\n",
    "    zeros_shape = triads_stores[set_nature]['zeros'].shape[0]\n",
    "    ones_shape = triads_stores[set_nature]['ones'].shape[0]\n",
    "    twos_shape = triads_stores[set_nature]['twos'].shape[0]\n",
    "    threes_shape = triads_stores[set_nature]['threes'].shape[0]\n",
    "\n",
    "    zeros = triads_stores[set_nature]['zeros'][np.random.randint(zeros_shape, size=min(small_samples, zeros_shape))]\n",
    "    ones = triads_stores[set_nature]['ones'][np.random.randint(ones_shape, size=min(small_samples, ones_shape))]\n",
    "    twos = triads_stores[set_nature]['twos'][np.random.randint(twos_shape, size=min(small_samples, twos_shape))]\n",
    "    threes = triads_stores[set_nature]['threes'][np.random.randint(threes_shape, size=min(small_samples, threes_shape))]\n",
    "\n",
    "    target_zeros = torch.zeros(zeros.shape[0])\n",
    "    target_ones = torch.ones(ones.shape[0])\n",
    "    target_twos = 2.0 * torch.ones(twos.shape[0])\n",
    "    target_threes = 3.0 * torch.ones(threes.shape[0])\n",
    "\n",
    "    out = torch.cat((zeros, ones, twos, threes), dim=0).view(-1,3).type(torch.long)\n",
    "    target = torch.cat((target_zeros, target_ones, target_twos, target_threes), dim=0).type(torch.long)\n",
    "    return out.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "validation_loss = 10000.0\n",
    "small_samples = 200\n",
    "for num_epoch in range(epochs):\n",
    "    mlp_optimizer.zero_grad()\n",
    "    input_ = torch.stack(hidden_samples_train)\n",
    "    input_ = input_.detach()\n",
    "    sampled, target = sample_triads('train', small_samples=small_samples)\n",
    "    loss = mlp.compute_loss(input_, sampled, target=target)\n",
    "    print(\"Training Loss: \", loss.item())\n",
    "    with torch.no_grad():\n",
    "        #Do Validation and check if validation loss has gone down\n",
    "        input_val = torch.stack(hidden_samples_validation)\n",
    "        input_val = input_val.detach()\n",
    "        sampled, target = sample_triads('validation', small_samples=small_samples)\n",
    "        compute_val_loss = mlp.compute_loss(input_val, sampled, target=target)\n",
    "        if compute_val_loss < validation_loss:\n",
    "            validation_loss = compute_val_loss\n",
    "            print(\"Validation Loss: \", validation_loss)\n",
    "            #Save Model\n",
    "            torch.save(mlp.state_dict(), mlp_model)\n",
    "    loss.backward()\n",
    "    mlp_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = StructMLP(node_set_size).to(device)\n",
    "mlp.load_state_dict(torch.load(mlp_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_samples = 200\n",
    "sampled_test, target_test = sample_triads('test', small_samples)\n",
    "\n",
    "\n",
    "t_test = target_test.to(\"cpu\").numpy()\n",
    "input_test = torch.stack(hidden_samples_test)\n",
    "input_test = input_test.detach()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = mlp.forward(input_test, sampled_test)\n",
    "    pred = F.log_softmax(test_pred, dim=1)\n",
    "pred = pred.detach().to(\"cpu\").numpy()\n",
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Micro F1 Score: \", f1_score(t_test, pred, average='micro'))\n",
    "print(\"Test Weighted F1 Score: \", f1_score(t_test, pred, average='weighted'))\n",
    "print(\"Test Accuracy Score: \", accuracy_score(t_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
